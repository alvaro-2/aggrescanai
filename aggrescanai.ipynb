{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08f5ac4",
   "metadata": {},
   "source": [
    "# AggrescanAI  \n",
    "Colab notebook user-friendy to calculate aggregation propensities using protein language models and deep neural networks.\n",
    "- Input: a protein sequence\n",
    "- Output: aggregation propensity profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Input sequence\n",
    "input_sequence = \"\"\"MDVFMKGLSKAKEGVVAAAEKTKQGVAEAAGKTKEGVLYVGSKTKEGVVHGVATVAEKTKEQVTNVGGAVVTGVTAVAQKTVEGAGSIAAATGFVKKDQLGKNEEGAPQEGILEDMPVDPDNEAYEMPSEEGYQDYEPEA\"\"\"  #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00718f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download models from HuggingFace\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "os.makedirs(\"nodels\", exist_ok=True)\n",
    "base_url = \"https://huggingface.co/alvaro-2/aggrescanai/tree/main\"\n",
    "model_names = [\n",
    "    f\"balanced_models/balanced_model_1_1_{i}.h5\" for i in range(1, 34)\n",
    "]\n",
    "for fname in model_names:\n",
    "    model_url = f\"{base_url}{fname}\"\n",
    "    model_path = f\"models/{os.path.basename(fname)}\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Downloading {fname}...\")\n",
    "        urllib.request.urlretrieve(model_url, model_path)\n",
    "\n",
    "# Download homology models\n",
    "homology_model_names = [\n",
    "    f\"homology_models/cpad_hotidp90_model_cv_{i}.h5\" for i in range(1, 6)\n",
    "]\n",
    "\n",
    "for fname in homology_model_names:\n",
    "    model_url = f\"{base_url}{fname}\"\n",
    "    model_path = f\"models/{os.path.basename(fname)}\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Downloading {fname}...\")\n",
    "        urllib.request.urlretrieve(model_url, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load models\n",
    "from tensorflow.keras.models import load_model\n",
    "models = [load_model(f\"models/{os.path.basename(fname)}\", compile= False) for fname in model_names]\n",
    "homology_models = [load_model(f\"models/{os.path.basename(fname)}\", compile= False) for fname in homology_model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4df4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate embedding representations\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load ProtT5 tokenizer and model\n",
    "transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = T5EncoderModel.from_pretrained(transformer_link, output_hidden_states=True).to(device).eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False, legacy=False)\n",
    "\n",
    "def generate_embeddings(sequence: str):\n",
    "    spaced = \" \".join(list(sequence))\n",
    "    ids = tokenizer(spaced, add_special_tokens=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=ids[\"input_ids\"], attention_mask=ids[\"attention_mask\"])\n",
    "    return out.last_hidden_state[0, :-1].cpu().numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
