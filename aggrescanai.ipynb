{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08f5ac4",
   "metadata": {},
   "source": [
    "# AggrescanAI  \n",
    "Colab notebook user-friendy to calculate aggregation propensities using protein language models and deep neural networks.\n",
    "- Input: a protein sequence\n",
    "- Output: aggregation propensity profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Input sequence\n",
    "input_sequence = \"MDVFMKGLSKAKEGVVAAAEKTKQGVAEAAGKTKEGVLYVGSKTKEGVVHGVATVAEKTKEQVTNVGGAVVTGVTAVAQKTVEGAGSIAAATGFVKKDQLGKNEEGAPQEGILEDMPVDPDNEAYEMPSEEGYQDYEPEA\"  #@param {type:\"string\"}\n",
    "# To dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'sequence': [input_sequence]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00718f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download models from HuggingFace\n",
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "base_url = \"https://huggingface.co/alvaro-2/aggrescanai/resolve/main\"\n",
    "model_names = [\n",
    "    f\"balanced_models/balanced_model_1_1_{i}.h5\" for i in range(1, 34)\n",
    "]\n",
    "# Download balanced models\n",
    "print(\"Downloading balanced models...\")\n",
    "for fname in tqdm(model_names):\n",
    "    model_url = f\"{base_url}/{fname}\"\n",
    "    os.makedirs(os.path.dirname(f\"models/balanced_models/\"), exist_ok=True)\n",
    "    model_path = f\"models/balanced_models/{os.path.basename(fname)}\"\n",
    "    if not os.path.exists(model_path):\n",
    "        urllib.request.urlretrieve(model_url, model_path)\n",
    "\n",
    "# Download homology models\n",
    "homology_model_names = [\n",
    "    f\"homology_models/cpad_hotidp90_model_cv_{i}.h5\" for i in range(1, 6)\n",
    "]\n",
    "\n",
    "print(\"Downloading homology models...\")\n",
    "for fname in tqdm(homology_model_names):\n",
    "    model_url = f\"{base_url}/{fname}\"\n",
    "    os.makedirs(os.path.dirname(f\"models/homology_models/\"), exist_ok=True)\n",
    "    model_path = f\"models/homology_models/{os.path.basename(fname)}\"\n",
    "    if not os.path.exists(model_path):\n",
    "        urllib.request.urlretrieve(model_url, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load models\n",
    "from tensorflow.keras.models import load_model\n",
    "models = [load_model(f\"models/balanced_models/{os.path.basename(fname)}\", compile= False) for fname in model_names]\n",
    "homology_models = [load_model(f\"models/homology_models/{os.path.basename(fname)}\", compile= False) for fname in homology_model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4df4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate embedding representations\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load ProtT5 tokenizer and model\n",
    "transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = T5EncoderModel.from_pretrained(transformer_link, output_hidden_states=True).to(device).eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False, legacy=False)\n",
    "\n",
    "def generate_embeddings(sequence: str):\n",
    "    spaced = \" \".join(list(sequence))\n",
    "    ids = tokenizer(spaced, add_special_tokens=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=ids[\"input_ids\"], attention_mask=ids[\"attention_mask\"])\n",
    "    return out.last_hidden_state[0, :-1].cpu().numpy()\n",
    "\n",
    "tqdm.pandas(desc=\"Generating embeddings\")\n",
    "df[\"embedding\"] = df[\"sequence\"].progress_map(generate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Apply soft-voting function\n",
    "import numpy as np\n",
    "def avg_probs(models, X, batch_size=32):\n",
    "    \"\"\"\n",
    "    models: list of keras.Model\n",
    "    X: numpy array de forma (n_samples, n_features)\n",
    "    devuelve: vector numpy (n_samples,) con la probabilidad media\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "    for m in models:\n",
    "        # model.predict devuelve shape (n_samples, 1) o (n_samples,)\n",
    "        p = m.predict(X, batch_size=batch_size)\n",
    "        p = p.reshape(-1)  # asegurar (n_samples,)\n",
    "        all_preds.append(p)\n",
    "    all_preds = np.stack(all_preds, axis=0)   # (n_models, n_samples)\n",
    "    return np.mean(all_preds, axis=0)         # (n_samples,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
